# -*- coding: utf-8 -*-
"""CBIRCWK2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mvx72YdGXqqnNLX6P5dtgLSvraDc1iCb

Animal image folder needs to be created and then upload 50 animal images into it
"""

import os #write and read files
import json #creating Json files
import numpy as np #image feature extraction
from numpy import linalg as LA
from skimage import feature # image processing
from os import listdir #file directory use
from PIL import Image # image processing tool
import matplotlib.pyplot as plt # to show images
import shutil #file directories
import cv2 #Open CV
from skimage.feature import graycomatrix, graycoprops #extract texture features
from skimage.color import rgb2gray #extract texture features
import base64 #encoding and decoding

#source path(raw folder)
src = '/content/Animals'

#destination(processed folder)
dest = '/content/Processed_animals'

#Created processed folder
os.makedirs('/content/Processed_animals', exist_ok=True)

#500 x 500 uniform size  and convert to RGB

#Loop through every file in source folder
for filename in os.listdir(src):
  if filename.endswith('.jpg'):
      try:
        #open image
        img_path = os.path.join(src, filename)
        with Image.open(img_path) as img:

          #Convert to RGB
          img_rgb = img.convert('RGB')

          #resize to 500 x 500
          img_resized = img_rgb.resize((500, 500), Image.Resampling.LANCZOS)

          #Convert PiL image to numpy array for OpenCV processing
          img_array = np.array(img_resized)

          #Denoise using Gaussian Blur
          img_denoised = cv2.GaussianBlur(img_array, (5, 5), 0)

          #Normalisation
          img_normalized = img_denoised.astype(np.float32) / 255.0

          #Convert back to uint8 format (0-255) for saving
          img_final = (img_normalized * 255).astype(np.uint8)

          #Convert back to PIL image for saving
          img_pil = Image.fromarray(img_final)

          #save to new folder
          save_path = os.path.join(dest, filename)
          img_pil.save(save_path, quality=95)  #High quality to preserve details
          print(f"Processed: {filename}")

      except Exception as e:
        print(f"Could not process {filename}: {e}")

print(f" All images preprocessed successfully!")
print(f" Saved to: {dest}")

"""## **Verify 500 x 500 Dimension and RGB mode**"""

testing = '/content/Processed_animals'

#Checking dimensions in folder
def check_image_sizes(testing, required_width = 500, required_height=500):

#iterating through files in folder
  for filename in os.listdir(testing):
    file_path = os.path.join(testing, filename)
    try:
        with Image.open(file_path) as img:#open image
          width, height = img.size
          #checking files dimension with required dimension
          if width == required_width and height == required_height:
            print(f"{filename} is {width}x{height} (OK)")
          else:
            print(f" {filename} is {width}x{height} (NOT 500x500)") #handling errors
    except Exception as e:
      print(f" Could not open {filename}: {e}")


check_image_sizes(testing)

#Checking all images are in RGB mode
verify_rgb = '/content/Processed_animals'
def check_image_modes(verify_rgb):




    #Iterating through files
    for filename in os.listdir(verify_rgb):
        file_path = os.path.join(verify_rgb, filename)
        try:
                with Image.open(file_path) as img:#open image file
                    print(f"{filename} → Mode: {img.mode}")
        except Exception as e:
                print(f"Could not open '{filename}': {e}")


check_image_modes(verify_rgb)

"""Image Annotation"""

#Processed images
#commented out for the moment
#Remove comment when submitting
"""
image_dir = "/content/Processed_animals"

#Output JSon File
output_dir = "/content/Animals_metadata/Animal_metadata"

#Different animal types
animal_types = ["mammal", "bird", "reptile"]

annotations = []

#Loop over all images in the folder
for filename in sorted(os.listdir(image_dir)):

  print(f"Image file: {filename}")
  print("Open this image from the Animals folder (left panel) to look at it.")
  #valid_type from ai is animal_types here

  #Ask user for animal_type
  while True:
    print("Animal types:", ", ".join(animal_types))
    animal_type = input("Enter animal_type: ").strip().lower()
    if animal_type in animal_types:
      break
    print("Please type one of the listed animal types.")

      #Keywords for image
  extra_keywords = input("Extra keywords (comma separated, can be empty):").strip()
  kw_list = [k.strip().lower() for k in extra_keywords.split(",") if k.strip()]

      #Keywords always include the animal type input
  keywords = [animal_type]
  keywords.extend(kw_list)

      #description
  description = input("Short description (1–2 sentences): ").strip()

      #Saving image info
  annotations.append({
          "filename": filename,
        "animal_type": animal_type,
        "keywords": keywords,
        "description": description
      })

      #Save all annotations to a JSON file
with open(output_dir, "w") as f:
        json.dump(annotations, f, indent=2)
print("Done! Saved", len(annotations), "items to", output_dir)

"""

"""Image Feature extraction:


*   First calculate the mean and norm(Keep RGB)
*   Second convert to gray scale


*   Third compute area
*   Fourth extract texture feature




"""

#calculate mean and norm intensity first


#Processed images folder
img_prep = "/content/Processed_animals"

#Animal Features folder
img_feat_dir ="/content/Animal_features"

#Json File with all features
img_feature = "/content/Animal_features/Animals_features.json" # Added .json extension


#Create folder
os.makedirs(img_feat_dir, exist_ok=True)

#Storing images with their features
img_extraction = []


#Loop Images in folder
for filename in sorted(os.listdir(img_prep)):
  #Building path to image file
  img_path = os.path.join(img_prep,filename)
  img = cv2.imread(img_path) #OpenCV reads img file returns as a numpy array

  #Handling errors
  if img is None:
    print(f"Warning: Could not load {filename}")
    continue #skip this image and move to next

  #Colour Conversion for RGB analysis
  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  mean_intensity = float(np.mean(img_rgb)) #mean intensity
  norm_intensity = float(np.linalg.norm(img_rgb)) #norm intensity


  print(f"Processing: {filename}")
  print(f"Image dimensions: {img_rgb.shape}") #Should show (500, 500,3)




  #Convert to GrayScale for texture features
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

  #Create binary image
  #cv2.threshold returns a tuple, we need the second element (the thresholded image)
  _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

  #Find contours
  # For OpenCV 4+, findContours returns two values
  contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

  #Calculate Area of Main Object
  if len(contours) > 0:
    largest_contour = max(contours, key=cv2.contourArea)
    area = float(cv2.contourArea(largest_contour))
    print(f"Found {len(contours)} contour(s)")
    print(f"Largest contour area: {area:.2f} pixels")
  else:
    area = float(gray.shape[0] * gray.shape[1])
    print(f"No contours found using full image area: {area:.2f} pixels")

  #Texture feature
  gray_uint8 = gray.astype(np.uint8)
  distances = [1]
  angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]
  glcm = graycomatrix(gray_uint8, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)

  #Calculate contrast from GLCM
  contrast = float(np.mean(graycoprops(glcm, 'contrast')))
  print(f"Texture contrast: {contrast:.4f}")

  #Organising all features into a dictionairy
  features = {
      'filename': filename,
#Intensity features
      'intensity_features': {
          'mean_intensity': round(mean_intensity, 4),
          'norm_intensity': round(norm_intensity, 4)
      },

      #Shape features
      'shape_features': {
          'area': round(area, 4)  # Size of object in square pixels
      },

      #Texture features(calculated on grayscale image)
      'texture_features': {
          'contrast': round(contrast, 4)  # Measure of texture roughness
      },


  }
  #creates a structured format

  #Add the image`s feature to the main list
  img_extraction.append(features)

#Save all extracted features to a JSON file after processing all images
with open(img_feature, "w") as f:
    json.dump(img_extraction, f, indent=2)

print(f"Done! Extracted features for {len(img_extraction)} images and saved to {img_feature}")

"""image Database"""

#Downloading preprocessed images
#!zip -r models.zip /content/Processed_animals

"""MongoDB installation"""

#MongoDB installation
!pip install pymongo
from pymongo import MongoClient

#Connect to MongoDB Atlas
uri = "mongodb+srv://Ayomide_20:Kelvin_810@cbir0.nw2qo7o.mongodb.net/?appName=Cbir0"
client = MongoClient(uri)

#Selecting database and collection
db = client["Retrieval_system"]
metadata_collection = db["animal_metadata"]

print("Connected to MongoDB Atlas")

#Path to Metadata Json file
metadata_file = "/content/Animal_features/Animal_metadata (1)"

#Loading Json file
with open(metadata_file, 'r', encoding='utf-8') as f:
  metadata_documents = json.load(f)

#Clear existing metadata to avoid duplicates
metadata_collection.delete_many({})


#Insert all metadata documents
result = metadata_collection.insert_many(metadata_documents)

print(f"Inserted {len(result.inserted_ids)} metadata documents")
print(f"Sample IDs: {result.inserted_ids[:3]}")

#Adding extracted features
print("Adding extracted features to metadata...")

#Path to features Json File
features_file = "Animal_features/Animals_features.json"

#Loading features JSON
with open(features_file, 'r', encoding='utf-8') as f:
  features_documents = json.load(f)


#Update each metadata document with its corresponding features
updated_count = 0
for feature_doc in features_documents:
  filename = feature_doc['filename']

  #extract the three features
  features_data = {
      'intensity_features': feature_doc['intensity_features'],
        'shape_features': feature_doc['shape_features'],
        'texture_features': feature_doc['texture_features']
  }

  #Update each metadata doc
  result = metadata_collection.update_one(
      {"filename": filename},
      {"$set": features_data}
  )

  if result.modified_count > 0:
    updated_count += 1

print(f"Updated {updated_count} documents with extracted features")

print("Loading processed animal images")

#Define the images collection reference
images_collection = db["animal_preprocessed"]

#Clear old images before inserting new ones
images_collection.delete_many({})


#Path to processed images folder
image_folder = "/content/Processed_animals"


#Checking if folder exists
if not os.path.exists(image_folder):
  print(f" ERROR: Folder '{image_folder}' not found!")
  print(f"Current directory contents: {os.listdir('.')}")
else:
  loaded_images = 0

  #Get all image files
  image_files = [f for f in os.listdir(image_folder)
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
  print(f"   Found {len(image_files)} images to process")

  #Process each image
  for filename in image_files:
    filepath = os.path.join(image_folder, filename)

    try:
      #Read image as binary
      with open(filepath, 'rb') as image_file:
          image_binary = image_file.read()

          #encode to base64
          encoded_image = base64.b64encode(image_binary).decode('utf-8')

          #Create document with image data
          image_doc = {
              "filename": filename,
              "image_data": encoded_image,
              "image_size_bytes": len(image_binary)
          }

          #Insert into collection
          images_collection.insert_one(image_doc)
          loaded_images += 1

          #Progress indicator
          if loaded_images % 10 == 0:
             print(f"   Processed {loaded_images}/{len(image_files)} images...")
    except Exception as e:
        print(f"    Error loading {filename}: {str(e)}")
  print(f"Successfully loaded {loaded_images} images")

client.close()
print("MongoDB connection closed")