# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/182OmL-dsQ9GVHHlOX6rmTo93ukgb-4fH
"""

import pandas as pd #import pandas

#load csv into dataframe
GP_reviews = pd.read_csv('/content/THE MISSION PRACTICE (Review Audit Reviews).csv')
GP_reviews

#Specifying columns
reviews = GP_reviews[['Star rating', 'Review content']]
reviews.head()

#Filter to only 50 rows without missing values
reviews_with_text = reviews.dropna(subset=['Review content'])#Review Content specified
fifty_reviews = reviews_with_text.head(50) #Specified 50 rows
fifty_reviews

#Verifying process sucessful
fifty_reviews.info()

#Display full text of Review content
pd.set_option('display.max_colwidth', None)
fifty_reviews

"""**Data preprocessing:**"""

#importing NLTK for data preprocessing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

#Set to english stopwords
stop_words = set(stopwords.words('english'))

#New dataframe to clean reivew content
Cleansing = fifty_reviews["Review content"]
Cleansing

#Applying lowercase and tokenize to all text
tokens = Cleansing.str.lower().apply(word_tokenize) #Convert object dtype to a string
tokens

#import Counter & itertools for efficient data processing pipelines
from collections import Counter
import itertools

#Flatten the list of tokenized words into a single list
all_words = list(itertools.chain.from_iterable(tokens)) #

#Count word frequencies
word_counts = Counter(all_words)

#Display the 10 most common words
print("Top 10 most common words:")
for word, count in word_counts.most_common(30):
    print(f"{word}: {count}")

#Removing stopwords
filtered_tokens = tokens.apply(lambda x: [word for word in x if word not in stop_words])

#Flatten the list of tokenized words into a single list
all_words = list(itertools.chain.from_iterable(filtered_tokens))

#Count word frequencies
word_counts = Counter(all_words)

#Display the 10 most common words
print("Top 10 most common words:")
for word, count in word_counts.most_common(30):
    print(f"{word}: {count}")

import string

Final_cleanse = filtered_tokens.apply(lambda review_tokens: [
    ''.join(char for char in word if char not in string.punctuation)
    for word in review_tokens
    if any(char.isalpha() for char in word) # Keep words that have at least one alphabet character after punctuation removal
])

#Flatten the list of tokenized words into a single list
all_words = list(itertools.chain.from_iterable(Final_cleanse))

#Count word frequencies
word_counts = Counter(all_words)


print("Top 10 most common words:")
for word, count in word_counts.most_common(30):
    print(f"{word}: {count}")

"""Text vectorisation"""

# Join tokens back into sentences
cleaned_texts = Final_cleanse.apply(lambda tokens: ' '.join(tokens))



print(f"Total reviews: {len(cleaned_texts)}")
print(f"Example cleaned review:")
print(f"Original tokens: {Final_cleanse.iloc[0][:10]}...")  # First 10 words
print(f"Joined text: {cleaned_texts.iloc[0][:100]}...")  # First 100 chars

"""Word frequency analysis"""

from collections import Counter
print("Word frequency analysis by star rating")

#Create a function to analyze words by star rating
def analyze_words_by_rating(df, tokens_column, rating_column):
  word_patterns = {}

  for rating in sorted(df[rating_column].unique()):

    #Get all tokens for this rating
    rating_tokens = tokens_column[df[rating_column] == rating]

    #Flatten to single list
    all_words = list(itertools.chain.from_iterable(rating_tokens))

    #Count frequencies
    word_freq = Counter(all_words)

    #Store top 15 words
    word_patterns[rating] = word_freq.most_common(15)

    #Print results
    print(f"{rating}-STAR REVIEWS - Top words:")
    for word, count in word_freq.most_common(10):
        print(f" '{word}': {count} times")

  return word_patterns

#Run analysis
word_patterns_by_rating = analyze_words_by_rating(
    fifty_reviews,
    Final_cleanse,
    'Star rating'
)

#Verifying distribution
print("Star Rating Distribution:")
print(fifty_reviews['Star rating'].value_counts().sort_index())
#This confirms in my dataset of 50 reviews it only has 1 star rating and 5 star rating

"""**Method 1: Bag of Words**"""

#Bag of Words vectorization

from sklearn.feature_extraction.text import CountVectorizer

#Initialize Bag of Words vectorizer
bow_vectorizer = CountVectorizer(
    max_features=100,
    min_df=2,
    lowercase=True,
    token_pattern=r'\b\w+\b'  #Exclude punctuation
)

#Fit and transform cleaned texts
bow_matrix = bow_vectorizer.fit_transform(cleaned_texts)

#Get vocabulary (the words the vectorizer learned)
bow_vocabulary = bow_vectorizer.get_feature_names_out()


#Convert sparse matrix to an array for easier handling
bow_array = bow_matrix.toarray()


print(f"Bag of Words Results:")
print(f"Vocabulary size: {len(bow_vocabulary)} words")
print(f"Matrix shape: {bow_array.shape}")
print(f"Meaning: {bow_array.shape[0]} reviews Ã— {bow_array.shape[1]} features")

print(f"Sample vocabulary (first 20 words):")
print(f"{list(bow_vocabulary[:20])}")


#Display an example of vectorization
print(f"Example - First review:")
print(f"Original text: '{cleaned_texts.iloc[0][:80]}...'")
print(f"Star rating: {fifty_reviews['Star rating'].iloc[0]}")
print(f"Vector (first 10 values): {bow_array[0][:10]}")

"""Method 2 : TF- idf Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer

#Initialize TF-IDF vectorizer (same parameters as BoW for comparison)
tfidf_vectorizer = TfidfVectorizer(
    max_features=100,
    min_df=2,
    lowercase=True,
    token_pattern=r'\b\w+\b'
)

#Fit and transform
tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)
tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()
tfidf_array = tfidf_matrix.toarray()


#Results
print(f"TF-IDF Results:")
print(f"Vocabulary size: {len(tfidf_vocabulary)} words")
print(f"Matrix shape: {tfidf_array.shape}")

print(f"Example - First review:")
print(f"Original text: '{cleaned_texts.iloc[0][:80]}...'")
print(f"TF-IDF vector (first 10 values): {tfidf_array[0][:10]}")

#Display important words in the first review
first_review_scores = list(zip(tfidf_vocabulary, tfidf_array[0]))
first_review_scores.sort(key=lambda x: x[1], reverse=True)

print(f"Top 5 most important words in first review (by TF-IDF):")
for word, score in first_review_scores[:5]:
  if score > 0: #Skips words with a score of 0
     print(f"  '{word}': {score:.4f}")

"""**Comparison of methods**"""

import numpy as np #for numerical operations.




print("Comparison: BoW vs TF-IDF Word Importance")


#Compare BoW and TF-IDF on 1 star reviews
one_star_mask = fifty_reviews['Star rating'] == 1
one_star_positions = [i for i, mask in enumerate(one_star_mask) if mask]

#BoW total across all 1-star reviews
bow_1star = np.sum([bow_array[i] for i in one_star_positions], axis=0)
bow_top_words = [(bow_vocabulary[i], bow_1star[i]) for i in range(len(bow_vocabulary))]
bow_top_words.sort(key=lambda x: x[1], reverse=True)

#TF-IDF average scores across all 1-star reviews
tfidf_1star = np.mean([tfidf_array[i] for i in one_star_positions], axis=0)
tfidf_top_words = [(tfidf_vocabulary[i], tfidf_1star[i]) for i in range(len(tfidf_vocabulary))]
tfidf_top_words.sort(key=lambda x: x[1], reverse=True)

print(f"1-STAR REVIEWS ({len(one_star_positions)} reviews analyzed):")

print(f"BoW - Most frequent words (total counts):")
for word, count in bow_top_words[:10]:
    if count > 0:
        print(f"  '{word}': {int(count)} times")

print(f"TF-IDF - Most important words (average scores):")
for word, score in tfidf_top_words[:10]:
    if score > 0:
        print(f"  '{word}': {score:.4f}")

#Compare BoW and TF-IDF on 5 star reviews
five_star_mask = fifty_reviews['Star rating'] == 5
five_star_positions = [i for i, mask in enumerate(five_star_mask) if mask]

#BoW top words
bow_5star = np.sum([bow_array[i] for i in five_star_positions], axis=0)
bow_top_words_5 = [(bow_vocabulary[i], bow_5star[i]) for i in range(len(bow_vocabulary))]
bow_top_words_5.sort(key=lambda x: x[1], reverse=True)#Builds list of (word,count) pairs in descending order

#TF-idf top words
tfidf_5star = np.mean([tfidf_array[i] for i in five_star_positions], axis=0)
tfidf_top_words_5 = [(tfidf_vocabulary[i], tfidf_5star[i]) for i in range(len(tfidf_vocabulary))]
tfidf_top_words_5.sort(key=lambda x: x[1], reverse=True)

print(f"5 STAR REVIEWS ({len(five_star_positions)} reviews analyzed):")

print(f"BoW  Most frequent words (total counts):")
for word, count in bow_top_words_5[:10]:
    if count > 0:
        print(f"  '{word}': {int(count)} times")

print(f"TF-idf Most important words (average scores):")
for word, score in tfidf_top_words_5[:10]:
    if score > 0:
        print(f"  '{word}': {score:.4f}")

#Manual check of review content based on rating
one_star_reviews = fifty_reviews[fifty_reviews['Star rating'] == 1]
print("Review content for 1-star ratings:")
for index, row in one_star_reviews.iterrows():
    print(f"Review ID: {index}\nContent: {row['Review content']}")

"""Metadata and labelling

Creating sentiment labels
"""

def create_sentiment_label(star_rating):
  if star_rating == 5:
    return 'positive'
  elif star_rating == 1:
    return 'negative'
  else:
      return 'neutral'#exception handling errors just incase


#Copy of dataframe
fifty_reviews_copy = fifty_reviews.copy()

#Apply the labeling function to create new column
fifty_reviews_copy['sentiment_label'] = fifty_reviews_copy['Star rating'].apply(create_sentiment_label)




#Verify labeling is correct
print(f"Verification - First 5 reviews:")
print(fifty_reviews_copy[['Star rating', 'sentiment_label']].head())


fifty_reviews = fifty_reviews_copy

"""Metadata creation"""

import json

#Get label distribution from your existing sentiment_label column
label_counts = fifty_reviews['sentiment_label'].value_counts()

#Metadata dictionairy
metadata = {

    #DataSet information
    "dataset_info": {
        "dataset_name": "Mission Practice GP Reviews - Sentiment Analysis",
        "source": "Google Reviews for The Mission Practice GP surgery",
        "total_reviews": len(fifty_reviews),
        "collection_method": "Exported using Phantom browser extension"
    },
    "vectorization_methods": {
        "Bag_of_words": "Each document is represented as a vector of word counts over the selected vocabulary.",
        "Tf_idf": "Each document is represented as a TF-IDF weighted vector, emphasizing words that are distinctive for that review."
    },
    "sentiment_labels": {
        "description": "Sentiment labels assigned to classify review sentiment",
        "labeling_method": "Star rating based classification",
        "label_column": "sentiment_label",
        "source_column": "Star rating",
        "labeling_rules": {
            "positive": {
                "criteria": "Star rating = 5",
                "count": int(label_counts.get('positive', 0))
            },
            "negative": {
                "criteria": "Star rating = 1",
                "count": int(label_counts.get('negative', 0))
            },
        },
    },
}

#Save metadata as JSON file
with open('text_vectorization_metadata.json', 'w') as f:
  json.dump(metadata, f, indent=2)


print("Metadata file created: text_vectorization_metadata.json")

"""Database Creation"""

#MongoDB installation
!pip install pymongo
from pymongo import MongoClient

#Connect to MongoDB Atlas
uri = "mongodb+srv://Ayomide_20:Kelvin_810@cbir0.nw2qo7o.mongodb.net/?appName=Cbir0"
client = MongoClient(uri)

#Selecting dabase and collection
db = client["NLP"]

#collections name
metadata_collections = db["doc_metadata"] #Containing the metadata
reviews_collections = db["doc_reviews"] # Containing text data and vectorization

print("Connected to NLP database")
print("Connected to collections: doc_metadata, doc_reviews")

#Clear existing metadata content
metadata_result = metadata_collections.delete_many({})
print(f"Deleted {metadata_result.deleted_count} old metadata documents")

#Clear existing reviews content
reviews_result = reviews_collections.delete_many({})
print(f"Deleted {reviews_result.deleted_count} old review documents")


#Loading metadata file
with open('text_vectorization_metadata.json', 'r') as f:
    metadata_content = json.load(f)

metadata_collections.insert_one(metadata_content)
print(f"Inserted metadata document")

#Review collections
review_documents = []

for i in range(len(fifty_reviews)):
  document = {
      #Basic identifiers
      'review_id': int(i),

      #Text data (original and preprocessed)
      'text_data': {
          'original': fifty_reviews['Review content'].iloc[i],
          'preprocessed': cleaned_texts.iloc[i]
      },

      #Vectorization(both methods)
      'vectorization': {
          'bow_vector': bow_array[i].tolist(),  # Convert numpy array to list
          'tfidf_vector': tfidf_array[i].tolist(),
          'vector_length': len(bow_array[i]),
          'vocabulary_size': len(bow_vocabulary)
      },

      #Labels and ratings
      'labels': {
          'sentiment_label': fifty_reviews['sentiment_label'].iloc[i],
          'star_rating': int(fifty_reviews['Star rating'].iloc[i])
      }

  }

  review_documents.append(document)

print(f"Prepared {len(review_documents)} review documents")

#Insert 50 review documents into database
result_reviews = reviews_collections.insert_many(review_documents)
print(f"Inserted {len(result_reviews.inserted_ids)} review documents")

client.close() #closing client